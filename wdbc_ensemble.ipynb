{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atributes:<br>\n",
    "```\n",
    "Paragraph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('wdbc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Class      1      2       3       4        5        6       7        8  \\\n",
       "0     M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.3001  0.14710   \n",
       "1     M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869  0.07017   \n",
       "2     M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974  0.12790   \n",
       "3     M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414  0.10520   \n",
       "4     M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980  0.10430   \n",
       "\n",
       "        9   ...        21     22      23      24      25      26      27  \\\n",
       "0  0.2419   ...     25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119   \n",
       "1  0.1812   ...     24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416   \n",
       "2  0.2069   ...     23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504   \n",
       "3  0.2597   ...     14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869   \n",
       "4  0.1809   ...     22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000   \n",
       "\n",
       "       28      29       30  \n",
       "0  0.2654  0.4601  0.11890  \n",
       "1  0.1860  0.2750  0.08902  \n",
       "2  0.2430  0.3613  0.08758  \n",
       "3  0.2575  0.6638  0.17300  \n",
       "4  0.1625  0.2364  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(axis = 1, columns = ['ID'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "data.Class = encoder.fit_transform(data.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, 1:].values\n",
    "y = data.Class.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging & Random Subspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [1.0, 0.5]\n",
    "models = [DecisionTreeClassifier(), Perceptron(tol = 0.0001, max_iter = 10000)]\n",
    "sizes = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]\n",
    "metrics = np.zeros((10,6,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "StratifiedKFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 features:\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') [[[0.99368687 0.94827586 0.94948152 0.93333333]\n",
      "  [0.99368687 0.94827586 0.94948152 0.93333333]\n",
      "  [0.99494949 0.94827586 0.94948152 0.93333333]\n",
      "  [0.99558081 0.96551724 0.97182532 0.95652174]\n",
      "  [0.99368687 0.94827586 0.94948152 0.93333333]\n",
      "  [0.99558081 0.94827586 0.94948152 0.93333333]]\n",
      "\n",
      " [[0.95138889 0.9137931  0.90313707 0.88372093]\n",
      "  [0.96843434 0.89655172 0.88975652 0.86363636]\n",
      "  [0.95328283 0.87931034 0.8660254  0.8372093 ]\n",
      "  [0.95075758 0.9137931  0.90313707 0.88372093]\n",
      "  [0.94633838 0.89655172 0.88975652 0.86363636]\n",
      "  [0.9520202  0.89655172 0.88975652 0.86363636]]\n",
      "\n",
      " [[0.98809524 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98875661 0.92982456 0.92439026 0.9047619 ]\n",
      "  [0.98941799 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98941799 0.92982456 0.91287093 0.9       ]\n",
      "  [0.98941799 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98941799 0.92982456 0.91287093 0.9       ]]\n",
      "\n",
      " [[0.96097884 0.94736842 0.9258201  0.92307692]\n",
      "  [0.96097884 0.94736842 0.9258201  0.92307692]\n",
      "  [0.96362434 0.96491228 0.95118973 0.95      ]\n",
      "  [0.96031746 0.94736842 0.9258201  0.92307692]\n",
      "  [0.96164021 0.94736842 0.9258201  0.92307692]\n",
      "  [0.95965608 0.94736842 0.9258201  0.92307692]]\n",
      "\n",
      " [[1.         0.98245614 0.97590007 0.97560976]\n",
      "  [1.         0.98245614 0.97590007 0.97560976]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         0.98245614 0.97590007 0.97560976]\n",
      "  [0.9973545  0.98245614 0.97590007 0.97560976]]\n",
      "\n",
      " [[0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.96491228 0.97182532 0.95454545]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]]\n",
      "\n",
      " [[0.96891534 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98611111 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98611111 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98544974 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98412698 0.92982456 0.91287093 0.9       ]\n",
      "  [0.98611111 0.94736842 0.93788572 0.92682927]]\n",
      "\n",
      " [[0.99591837 0.98214286 0.97590007 0.97560976]\n",
      "  [0.9952381  0.98214286 0.97590007 0.97560976]\n",
      "  [0.99591837 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99183673 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99319728 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99183673 0.98214286 0.97590007 0.97560976]]\n",
      "\n",
      " [[0.99795918 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99591837 0.96428571 0.96185761 0.95238095]\n",
      "  [1.         0.98214286 0.98561076 0.97674419]\n",
      "  [0.99863946 0.96428571 0.96185761 0.95238095]\n",
      "  [0.99863946 0.96428571 0.96185761 0.95238095]\n",
      "  [0.99863946 0.96428571 0.96185761 0.95238095]]\n",
      "\n",
      " [[1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         0.98214286 0.97590007 0.97560976]\n",
      "  [1.         1.         1.         1.        ]]]\n",
      "\n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=10000, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=0.0001, verbose=0, warm_start=False) [[[0.95138889 0.94827586 0.94012679 0.93023256]\n",
      "  [0.9520202  0.93103448 0.91632225 0.9047619 ]\n",
      "  [0.95328283 0.94827586 0.94012679 0.93023256]\n",
      "  [0.95265152 0.94827586 0.94012679 0.93023256]\n",
      "  [0.9520202  0.93103448 0.91632225 0.9047619 ]\n",
      "  [0.94949495 0.93103448 0.91632225 0.9047619 ]]\n",
      "\n",
      " [[0.95643939 0.94827586 0.92932038 0.92682927]\n",
      "  [0.95454545 0.93103448 0.90453403 0.9       ]\n",
      "  [0.96338384 0.93103448 0.90453403 0.9       ]\n",
      "  [0.96212121 0.94827586 0.92932038 0.92682927]\n",
      "  [0.96275253 0.93103448 0.90453403 0.9       ]\n",
      "  [0.96338384 0.94827586 0.92932038 0.92682927]]\n",
      "\n",
      " [[0.96560847 0.87719298 0.86143107 0.82926829]\n",
      "  [0.96957672 0.89473684 0.88640526 0.85714286]\n",
      "  [0.96494709 0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.96494709 0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.96759259 0.87719298 0.86143107 0.82926829]\n",
      "  [0.96230159 0.87719298 0.87287156 0.8372093 ]]\n",
      "\n",
      " [[0.99338624 0.94736842 0.93788572 0.92682927]\n",
      "  [0.99338624 0.94736842 0.93788572 0.92682927]\n",
      "  [0.99206349 0.94736842 0.93788572 0.92682927]\n",
      "  [0.99338624 0.94736842 0.9484044  0.93023256]\n",
      "  [0.99272487 0.94736842 0.9484044  0.93023256]\n",
      "  [0.99206349 0.94736842 0.93788572 0.92682927]]\n",
      "\n",
      " [[0.96759259 0.9122807  0.89973541 0.87804878]\n",
      "  [0.96957672 0.9122807  0.89973541 0.87804878]\n",
      "  [0.96825397 0.9122807  0.89973541 0.87804878]\n",
      "  [0.96891534 0.9122807  0.89973541 0.87804878]\n",
      "  [0.96891534 0.9122807  0.89973541 0.87804878]\n",
      "  [0.96560847 0.89473684 0.88640526 0.85714286]]\n",
      "\n",
      " [[0.95568783 0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.96031746 0.87719298 0.86143107 0.82926829]\n",
      "  [0.95238095 0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.95502646 0.89473684 0.88640526 0.85714286]\n",
      "  [0.95238095 0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.94775132 0.87719298 0.87287156 0.8372093 ]]\n",
      "\n",
      " [[0.98478836 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98015873 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98280423 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98544974 0.92982456 0.92439026 0.9047619 ]\n",
      "  [0.98082011 0.92982456 0.92439026 0.9047619 ]\n",
      "  [0.98280423 0.96491228 0.95118973 0.95      ]]\n",
      "\n",
      " [[0.93129252 0.875      0.84756554 0.82051282]\n",
      "  [0.93197279 0.875      0.84756554 0.82051282]\n",
      "  [0.94693878 0.89285714 0.8603116  0.84210526]\n",
      "  [0.92244898 0.89285714 0.8603116  0.84210526]\n",
      "  [0.92380952 0.875      0.84756554 0.82051282]\n",
      "  [0.92517007 0.875      0.84756554 0.82051282]]\n",
      "\n",
      " [[0.93537415 0.85714286 0.85714286 0.81818182]\n",
      "  [0.92585034 0.82142857 0.82807867 0.7826087 ]\n",
      "  [0.92517007 0.85714286 0.85714286 0.81818182]\n",
      "  [0.92585034 0.85714286 0.85714286 0.81818182]\n",
      "  [0.92721088 0.83928571 0.83299313 0.79069767]\n",
      "  [0.92244898 0.85714286 0.83462485 0.8       ]]\n",
      "\n",
      " [[0.9707483  0.92857143 0.91249825 0.9       ]\n",
      "  [0.96734694 0.92857143 0.91249825 0.9       ]\n",
      "  [0.96938776 0.89285714 0.8603116  0.84210526]\n",
      "  [0.9707483  0.89285714 0.88525334 0.85714286]\n",
      "  [0.96802721 0.89285714 0.8603116  0.84210526]\n",
      "  [0.96666667 0.89285714 0.88525334 0.85714286]]]\n",
      "0.5 features:\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') [[[0.99747475 0.98275862 0.9860133  0.97777778]\n",
      "  [0.99873737 0.98275862 0.9860133  0.97777778]\n",
      "  [0.99873737 0.98275862 0.9860133  0.97777778]\n",
      "  [0.99810606 0.98275862 0.9860133  0.97777778]\n",
      "  [0.99873737 0.96551724 0.96334329 0.95454545]\n",
      "  [0.99747475 0.98275862 0.9860133  0.97777778]]\n",
      "\n",
      " [[0.96464646 0.9137931  0.89188259 0.87804878]\n",
      "  [0.96590909 0.89655172 0.87904907 0.85714286]\n",
      "  [0.96338384 0.89655172 0.87904907 0.85714286]\n",
      "  [0.96969697 0.89655172 0.87904907 0.85714286]\n",
      "  [0.94570707 0.9137931  0.89188259 0.87804878]\n",
      "  [0.94065657 0.9137931  0.89188259 0.87804878]]\n",
      "\n",
      " [[0.98743386 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98677249 0.92982456 0.91287093 0.9       ]\n",
      "  [0.98677249 0.92982456 0.91287093 0.9       ]\n",
      "  [0.98875661 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98677249 0.92982456 0.91287093 0.9       ]\n",
      "  [0.98677249 0.94736842 0.93788572 0.92682927]]\n",
      "\n",
      " [[0.96693122 0.96491228 0.95118973 0.95      ]\n",
      "  [0.96626984 0.96491228 0.95118973 0.95      ]\n",
      "  [0.96494709 0.96491228 0.95118973 0.95      ]\n",
      "  [0.96428571 0.94736842 0.9258201  0.92307692]\n",
      "  [0.96428571 0.94736842 0.9258201  0.92307692]\n",
      "  [0.97420635 0.94736842 0.9258201  0.92307692]]\n",
      "\n",
      " [[1.         1.         1.         1.        ]\n",
      "  [1.         0.98245614 0.97590007 0.97560976]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         0.98245614 0.97590007 0.97560976]\n",
      "  [0.99867725 0.98245614 0.97590007 0.97560976]\n",
      "  [1.         0.98245614 0.97590007 0.97560976]]\n",
      "\n",
      " [[0.99603175 0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.99867725 0.98245614 0.9860133  0.97674419]\n",
      "  [0.9973545  0.98245614 0.9860133  0.97674419]\n",
      "  [0.99867725 0.98245614 0.9860133  0.97674419]]\n",
      "\n",
      " [[0.98809524 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98875661 0.96491228 0.95118973 0.95      ]\n",
      "  [0.98875661 0.96491228 0.95118973 0.95      ]\n",
      "  [0.96693122 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98875661 0.96491228 0.95118973 0.95      ]\n",
      "  [0.99007937 0.96491228 0.95118973 0.95      ]]\n",
      "\n",
      " [[0.9952381  0.98214286 0.97590007 0.97560976]\n",
      "  [0.99795918 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99727891 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99863946 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99863946 0.98214286 0.97590007 0.97560976]\n",
      "  [0.9952381  0.98214286 0.97590007 0.97560976]]\n",
      "\n",
      " [[0.99863946 0.98214286 0.98561076 0.97674419]\n",
      "  [0.99727891 0.96428571 0.96185761 0.95238095]\n",
      "  [0.99727891 0.96428571 0.96185761 0.95238095]\n",
      "  [0.99727891 0.96428571 0.96185761 0.95238095]\n",
      "  [0.99659864 0.98214286 0.97590007 0.97560976]\n",
      "  [0.99863946 0.96428571 0.96185761 0.95238095]]\n",
      "\n",
      " [[1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]\n",
      "  [1.         1.         1.         1.        ]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perceptron(alpha=0.0001, class_weight=None, eta0=1.0, fit_intercept=True,\n",
      "      max_iter=10000, n_iter=None, n_jobs=1, penalty=None, random_state=0,\n",
      "      shuffle=True, tol=0.0001, verbose=0, warm_start=False) [[[0.95328283 0.9137931  0.90313707 0.88372093]\n",
      "  [0.93434343 0.86206897 0.82877541 0.8       ]\n",
      "  [0.92676768 0.86206897 0.82877541 0.8       ]\n",
      "  [0.94760101 0.89655172 0.87904907 0.85714286]\n",
      "  [0.94191919 0.87931034 0.8660254  0.8372093 ]\n",
      "  [0.95833333 0.87931034 0.85428214 0.82926829]]\n",
      "\n",
      " [[0.93623737 0.86206897 0.81417368 0.78947368]\n",
      "  [0.93939394 0.82758621 0.82572282 0.7826087 ]\n",
      "  [0.93308081 0.87931034 0.8660254  0.8372093 ]\n",
      "  [0.9375     0.9137931  0.87904907 0.87179487]\n",
      "  [0.90214646 0.79310345 0.79772404 0.75      ]\n",
      "  [0.90530303 0.77586207 0.75377836 0.69767442]]\n",
      "\n",
      " [[0.9728836  0.9122807  0.87287156 0.86486486]\n",
      "  [0.96957672 0.87719298 0.83333333 0.81081081]\n",
      "  [0.96230159 0.87719298 0.84827868 0.82051282]\n",
      "  [0.96031746 0.89473684 0.88640526 0.85714286]\n",
      "  [0.9702381  0.84210526 0.85371891 0.80851064]\n",
      "  [0.9728836  0.9122807  0.88715108 0.87179487]]\n",
      "\n",
      " [[0.96693122 0.92982456 0.91287093 0.9       ]\n",
      "  [0.96494709 0.89473684 0.89679028 0.86363636]\n",
      "  [0.98082011 0.94736842 0.93788572 0.92682927]\n",
      "  [0.96230159 0.89473684 0.89679028 0.86363636]\n",
      "  [0.9702381  0.92982456 0.92439026 0.9047619 ]\n",
      "  [0.97949735 0.94736842 0.93788572 0.92682927]]\n",
      "\n",
      " [[0.96296296 0.89473684 0.89679028 0.86363636]\n",
      "  [0.95634921 0.9122807  0.91069483 0.88372093]\n",
      "  [0.96031746 0.89473684 0.87438565 0.85      ]\n",
      "  [0.96296296 0.89473684 0.87438565 0.85      ]\n",
      "  [0.96693122 0.87719298 0.88266671 0.84444444]\n",
      "  [0.9702381  0.85964912 0.86831345 0.82608696]]\n",
      "\n",
      " [[0.97685185 0.89473684 0.87438565 0.85      ]\n",
      "  [0.97751323 0.89473684 0.87438565 0.85      ]\n",
      "  [0.9755291  0.9122807  0.92008741 0.88888889]\n",
      "  [0.9728836  0.87719298 0.87287156 0.8372093 ]\n",
      "  [0.97619048 0.89473684 0.91287093 0.875     ]\n",
      "  [0.98412698 0.9122807  0.91069483 0.88372093]]\n",
      "\n",
      " [[0.99272487 0.94736842 0.93788572 0.92682927]\n",
      "  [0.98941799 0.92982456 0.94280904 0.91304348]\n",
      "  [0.99404762 0.92982456 0.94280904 0.91304348]\n",
      "  [0.98809524 0.92982456 0.93435318 0.90909091]\n",
      "  [0.99206349 0.89473684 0.91287093 0.875     ]\n",
      "  [0.99272487 0.92982456 0.91287093 0.9       ]]\n",
      "\n",
      " [[0.9537415  0.92857143 0.92361314 0.9047619 ]\n",
      "  [0.95442177 0.94642857 0.93750283 0.92682927]\n",
      "  [0.97210884 0.89285714 0.84515425 0.83333333]\n",
      "  [0.96938776 0.91071429 0.8867889  0.87179487]\n",
      "  [0.95986395 0.96428571 0.95118973 0.95      ]\n",
      "  [0.96054422 0.94642857 0.93750283 0.92682927]]\n",
      "\n",
      " [[0.94761905 0.85714286 0.865829   0.82608696]\n",
      "  [0.94897959 0.89285714 0.9035079  0.86956522]\n",
      "  [0.95442177 0.875      0.88832181 0.85106383]\n",
      "  [0.94829932 0.83928571 0.85714286 0.81632653]\n",
      "  [0.94965986 0.85714286 0.87287156 0.83333333]\n",
      "  [0.9537415  0.82142857 0.84112008 0.8       ]]\n",
      "\n",
      " [[0.97210884 0.92857143 0.89973541 0.89473684]\n",
      "  [0.97414966 0.92857143 0.91249825 0.9       ]\n",
      "  [0.97142857 0.92857143 0.91249825 0.9       ]\n",
      "  [0.97210884 0.94642857 0.93750283 0.92682927]\n",
      "  [0.96462585 0.92857143 0.89973541 0.89473684]\n",
      "  [0.96870748 0.92857143 0.89973541 0.89473684]]]\n"
     ]
    }
   ],
   "source": [
    "for feature in features:\n",
    "    print(feature, \"features:\")\n",
    "    for model in models:\n",
    "        fold = 0\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            for i, size in enumerate(sizes):\n",
    "                # All data points\n",
    "                bg = BaggingClassifier(model,\n",
    "                                         max_samples = size,\n",
    "                                         max_features = feature,\n",
    "                                         n_estimators = 100,\n",
    "                                         random_state = 42)\n",
    "                bg.fit(X_train, y_train)\n",
    "\n",
    "                metrics[fold, i, :] = [roc_auc_score(y_test, bg.predict_proba(X_test)[:,1]),\n",
    "                      bg.score(X_test, y_test),\n",
    "                      geometric_mean_score(y_test, bg.predict(X_test)),\n",
    "                      f1_score(y_test, bg.predict(X_test))]\n",
    "            fold += 1\n",
    "        print()\n",
    "        print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
